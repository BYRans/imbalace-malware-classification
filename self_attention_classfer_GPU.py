from __future__ import print_function
from __future__ import absolute_import
from numpy.random import seed
import matplotlib.pyplot as plt
import time
from keras.preprocessing import sequence
from keras.utils import to_categorical, print_summary
from keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix
from malware_classification.Self_Attention import Self_Attention_Layer
from keras.models import Model
from keras.layers import *
import os
import tensorflow as tf
import keras
from keras_self_attention import SeqSelfAttention
from keras_multi_head import MultiHeadAttention
from keras import backend as K
from keras.layers import Dense
from sklearn.model_selection import StratifiedKFold
import numpy as np
seed = 7
np.random.seed(seed)
from sklearn.utils import class_weight as cw



def precision(y_true, y_pred):
    # Calculates the precision
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision


def recall(y_true, y_pred):
    # Calculates the recall
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall


def fbeta_score(y_true, y_pred, beta=1):
    # Calculates the F score, the weighted harmonic mean of precision and recall.
    if beta < 0:
        raise ValueError('The lowest choosable beta is zero (only precision).')

    # If there are no true positives, fix the F score at 0 like sklearn.
    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:
        return 0

    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    bb = beta ** 2
    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())
    return fbeta_score


def fmeasure(y_true, y_pred):
    # Calculates the f-measure, the harmonic mean of precision and recall.
    return fbeta_score(y_true, y_pred, beta=1)


# focal loss with multi label
def focal_loss(classes_num, gamma=2., e=0.1):
    # classes_num contains sample number of each classes
    def focal_loss_fixed(target_tensor, prediction_tensor):
        '''
        prediction_tensor is the output tensor with shape [None, 100], where 100 is the number of classes
        target_tensor is the label tensor, same shape as predcition_tensor
        '''
        import tensorflow as tf
        from tensorflow.python.ops import array_ops
        from keras import backend as K

        # 1# get focal loss with no balanced weight which presented in paper function (4)
        zeros = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)
        one_minus_p = array_ops.where(tf.greater(target_tensor, zeros),
                                      target_tensor - prediction_tensor, zeros)
        FT = -1 * (one_minus_p ** gamma) * tf.log(tf.clip_by_value(prediction_tensor, 1e-8, 1.0))

        # 2# get balanced weight alpha
        classes_weight = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)

        total_num = float(sum(classes_num))
        classes_w_t1 = [total_num / ff for ff in classes_num]
        sum_ = sum(classes_w_t1)
        classes_w_t2 = [ff / sum_ for ff in classes_w_t1]  # scale
        classes_w_tensor = tf.convert_to_tensor(classes_w_t2, dtype=prediction_tensor.dtype)
        classes_weight += classes_w_tensor

        alpha = array_ops.where(tf.greater(target_tensor, zeros), classes_weight, zeros)

        # 3# get balanced focal loss
        balanced_fl = alpha * FT
        balanced_fl = tf.reduce_mean(balanced_fl)

        # 4# add other op to prevent overfit
        # reference : https://spaces.ac.cn/archives/4493
        nb_classes = len(classes_num)
        fianal_loss = (1 - e) * balanced_fl + e * K.categorical_crossentropy(
            K.ones_like(prediction_tensor) / nb_classes, prediction_tensor)

        return fianal_loss

    return focal_loss_fixed


# this plots a confusion matrix
def plot_cm(confusion,pic_path,class_num=9):
    plt.imshow(confusion, cmap=plt.cm.Blues)

    # ticks 坐标轴的坐标点
    # label 坐标轴标签说明
    indices = range(len(confusion))
    # 第一个是迭代对象，表示坐标的显示顺序，第二个参数是坐标轴显示列表
    # plt.xticks(indices, [0, 1, 2])
    # plt.yticks(indices, [0, 1, 2])

    if class_num == 9:
        plt.xticks(indices, ['R', 'L', 'K_v3', 'V', 'S', 'T', 'K_v1', 'O.ACY', 'G'])
        plt.yticks(indices, ['Ramnit', 'Lollipop', 'Kelihos_ver3', 'Vundo', 'Simda', 'Tracur', 'Kelihos_ver1', 'Obfuscator.ACY', 'Gatak'])
    else:
        plt.xticks(indices, ['R', 'L', 'K_v3', 'V', 'T', 'K_v1', 'O.ACY', 'G'])
        plt.yticks(indices, ['Ramnit', 'Lollipop', 'Kelihos_ver3', 'Vundo', 'Tracur',
                             'Kelihos_ver1', 'Obfuscator.ACY', 'Gatak'])

    plt.colorbar()

    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')

    # 显示数据
    for first_index in range(len(confusion)):  # 第几行
        for second_index in range(len(confusion[first_index])):  # 第几列
            plt.text(first_index, second_index, confusion[first_index][second_index],ha='center')
    plt.savefig(pic_path)
    plt.show()



def load_npz_data(file_path):
    f = np.load(file_path)
    x, y = f['x'], f['y']
    f.close()
    return x, y

def create_model(maxlen=20000,embedding=256,salayer=256,class_num=9):
    print('Build model...')
    S_inputs = Input(shape=(maxlen,), dtype='int32')
    embeddings = Embedding(maxlen, embedding)(S_inputs)
    O_seq = Self_Attention_Layer(salayer)(embeddings)
    O_seq = GlobalAveragePooling1D()(O_seq)
    O_seq = Dropout(0.05)(O_seq)
    outputs = Dense(class_num, activation='softmax')(O_seq)
    model = Model(inputs=S_inputs, outputs=outputs)
    print(model.summary())
    return model

def create_self_attention_model(maxlen=20000,embedding=256,salayer=256):

    model = keras.models.Sequential()
    model.add(keras.layers.Embedding(input_dim=maxlen,
                                     output_dim=embedding))
    model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=salayer,
                                                           return_sequences=True)))
    model.add(SeqSelfAttention(attention_activation='sigmoid'))
    model.add(keras.layers.Dense(units=9, activation='softmax'))
    print(model.summary())
    return model

def create_muti_head_self_attention_model(maxlen=20000,embedding=256,salayer=256):
    S_inputs = Input(shape=(maxlen,), dtype='int32')
    embeddings = Embedding(maxlen, embedding)(S_inputs)
    O_seq = MultiHeadAttention(
        head_num=3,
        name='Multi-Head',
    )(embeddings)
    O_seq = Flatten()(O_seq)
    outputs = Dense(9, activation='softmax')(O_seq)
    model = Model(inputs=S_inputs, outputs=outputs)

    print(model.summary())
    return model
def train_and_test(gamma=2., loss_type="base", batch_size=2, epochs=12, validation_split=0.15,embedding = 128,salayer = 150,imbalance=True,maxlen = 15000,num_words=10000,
                   class_weight={0: 1., 1: 1., 2: 1., 3: 1., 4: 1., 5: 1., 6: 1., 7: 1., 8: 1.}):



     # cut texts after this number of words (among top max_features most common words), used in pad_sequences (right after load data, before any modeling)



    print('Loading data...')

    # num_words' default is None, which is taken to mean that all unique integers should be kept.

    if imbalance:
        x_y_data_path = "E:/Microsoft/RunningOutput/np_train_data/hex_text_np_x_y_data.npz"
        class_num = 9
    else:
        if num_words == 10000:
            x_y_data_path = "G:/数据集/Microsoft/RunningOutput/np_train_data/hex_text_np_x_y_data_10000_b.npz"
            class_num = 8
        elif num_words == 20000:
            x_y_data_path = "G:/数据集/Microsoft/RunningOutput/np_train_data/hex_text_np_x_y_data_20000_b.npz"
            class_num = 8
    print("reading data from ", x_y_data_path)
    x, y = load_npz_data(x_y_data_path)
    print('x sequences len: ',len(x))
    print('y sequences len: ',len(y))

    test_or_not = False
    if test_or_not:
        epochs = 1
        x = x[:1000]
        y = y[:1000]


    print('Pad sequences (samples x time)')
    x = sequence.pad_sequences(x, maxlen=maxlen)
    print('x shape:', x.shape)

    # We have classes from 1 to 9, which is 9 classes, but to_categorical will make an array with spots from 0 to max_class, so we subtract 1 such that our classes are 0 to 8 and we can use 9 classes.
    y -= 1
    y = to_categorical(y, class_num)
    print('y shape:', y.shape)

    fw = open('G:/数据集/Microsoft/RunningOutput/scores/scores-balance-K-Folder-20200322.txt', 'a+', encoding='utf-8')
    saveScore = "\n\n\n===============" + loss_type + "====================\nepochs=" + str(
        epochs) + ",validation_split=" + str(validation_split) + ",embedding=" + str(embedding) + ",salayers=" +  str(salayer) + ",maxLen=" + str(maxlen) + ",Num_words=" + str(num_words)
    if loss_type == "focal":
        saveScore += "\ngamma: " + str(gamma)
    elif loss_type == "weight":
        saveScore += "\nweight:" + str(class_weight)
    fw.write(saveScore)
    fw.flush()

    # define 10-fold cross validation test harness
    kfold = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed)
    loss = []
    accs = []
    pres = []
    recs = []
    fscs = []
    cms = np.zeros((class_num,class_num))
    k=0
    print(y)
    print(y.argmax(1))

    for train, test in kfold.split(x, y.argmax(1)):
        k += 1
        # create model
        model = create_model(maxlen=maxlen,embedding=embedding,salayer=salayer,class_num=class_num)
        # Compile model
        if loss_type == "base" or loss_type == "weight":
            model.compile(loss="categorical_crossentropy",
                          optimizer='adam',
                          metrics=['accuracy', precision, recall, fmeasure])
        else: # loss_type == "focal":
            # classes_num = [1541, 2478, 2942, 475, 42, 751, 398, 1228, 1013]
            classes_num = [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]
            model.compile(loss=[focal_loss(classes_num, gamma=gamma)],
                          optimizer='adam',
                          metrics=['accuracy', precision, recall, fmeasure])
        es = EarlyStopping(monitor="val_accuracy", min_delta=0.0001, patience=5,
                           mode='max')  # meaning of this: when val_acc stops increasing by more than min_delta, we stop at current epoch. patience had default of 0. patience is the number of epochs with no improvement after which training will be stopped. the definition of improvement seems to be our min delta rule. We wonder if this will make less variation in accuracies when running lots of identical jobs

        # Fit the model
        if loss_type == "weight":
            model.fit(x[train], y[train],
                      batch_size=batch_size, class_weight=class_weight,
                      epochs=epochs,
                      validation_split=validation_split, callbacks=[es])
        else:
            model.fit(x[train], y[train],
                      batch_size=batch_size,
                      epochs=epochs,
                      validation_split=validation_split, callbacks=[es])
        # evaluate the model
        score = model.evaluate(x[test], y[test], batch_size=batch_size)
        loss.append(score[0])
        accs.append(score[1])
        pres.append(score[2])
        recs.append(score[3])
        fscs.append(score[4])
        cm_k = confusion_matrix(y[test].argmax(1), (model.predict(x[test],batch_size=batch_size)).argmax(1))
        print(cm_k)
        cms = cms + cm_k

        print("------",k,"/10------")
        print(model.metrics_names)
        print('test Scores: ', score)

        saveScore = "\n--" + str(k) + "/10--\n" + str(model.metrics_names) + '\nTest Scores: ' + str(score) + "\n"
        saveScore += str(cm_k)
        fw.write(saveScore)
        fw.flush()
    final_scores = f"\n\n------Final Scores------\nMean Loss: {np.mean(loss)}  \nMean Accuracy: {np.mean(accs)}  \nMean Precision: {np.mean(pres)}  \nMean Recall: {np.mean(recs)} \nMean F-Score: {np.mean(fscs)} \n\n"
    print(final_scores)
    saveScore = final_scores
    saveScore += str(cms.astype('int')) + "\n\n"
    fw.write(saveScore)
    fw.flush()

    cnf_matrix = cms.astype('float') / cms.sum(axis=1)[:, np.newaxis]  # 归一化
    cnf_matrix = cnf_matrix * 100
    cnf_matrix = np.around(cnf_matrix, decimals=2)
    print(cnf_matrix)
    saveScore = str(cnf_matrix)

    fw.write(saveScore)
    fw.close()

    c_time = time.strftime("%Y-%m-%d__%H_%M_%S", time.localtime())
    pic_path = "G:/数据集/Microsoft/RunningOutput/cmpic/" + loss_type + "_"+c_time+".png"
    plot_cm(cnf_matrix,pic_path,class_num)
    pic_cms_path = "G:/数据集/Microsoft/RunningOutput/cmpic/" + loss_type + "_Count_"+c_time+".png"
    plot_cm(cms, pic_cms_path, class_num)

if __name__ == '__main__':
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    gpu_options = tf.GPUOptions(allow_growth=True)
    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))
    keras.backend.set_session(sess)

    train_and_test( loss_type="base", validation_split=0.2, embedding=128, salayer=128,imbalance=False,maxlen = 20000,num_words=20000,batch_size=1)
    train_and_test(loss_type="base", validation_split=0.2, embedding=64, salayer=128, imbalance=False, maxlen=20000,
                   num_words=20000, batch_size=1)





