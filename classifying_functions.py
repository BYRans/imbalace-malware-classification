# written by Viktor Zenkov in 2018

# this file contains functions for reading in training data and processing it and splitting off the training data

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from keras.utils.data_utils import get_file

from keras.preprocessing.sequence import _remove_long_seq

import numpy as np
import json
import warnings
import os
import math
import time
from sklearn.model_selection import train_test_split


# this function loads the data.
# path is "where to cache the data (relative to `~/.keras/dataset`)"
# num_words is the number of unique words to keep
# skip_top is the number of most frequent words to skip (for example, if "the" was most common in a context this might be altered to skip "the")
# maxlen is the length of each sequence to keep from each file (keep the first maxlen words)
# seed is the random seed to use for shuffling the data
# start_char is the number which is inserted at the beginning of each sequence (file)
# oov_char is the number to replace words lost because they're greater than num_words.
# index_from is used to push increase all the integers to make room for the start_char and oov_char integers
# numOrHex determines if we want to read in text (True) or hex (False) data

# useful parameters to alter are num_words, maxlen, seed, and numOrHex
def load_data(num_words=None, skip_top=0,
              maxlen=None, seed=113,
              start_char=1, oov_char=2, index_from=3, **kwargs):

    # if we want to use only a few files, set limitNumFiles to True
    numFiles = 1000
    limitNumFiles = False


    # the paths of the text or hex integer files
    f_path = 'G:/数据集/Microsoft/RunningOutput/int_hex_text_all'

    # get the files in order and keep only txt files
    allFileNames = sorted(os.listdir(f_path))
    allFileNames = [f_name for f_name in allFileNames if f_name.endswith('.txt')]
    xs = []

    print("starting reading files at ", time.time())

    counter = 0

    # read in each file and puts its contents into an element of xs
    for f_name in allFileNames:

        # small number of files code:
        counter += 1
        if (limitNumFiles and counter > numFiles):
            break

        f = open(f_path + '/' + f_name, encoding='latin-1')
        tempS = f.read()
        xs.append(tempS)

    print("total read files count is: ", len(xs))
    print("xs 0 len:", len(xs))

    # read in the file labels and create a dictionary
    dictionaryLabels = {}
    with open('G:/数据集/Microsoft/trainLabels.csv', 'r') as dicFile:
        for lineEntry in dicFile:
            bothWords = lineEntry.split(',')
            dictionaryLabels[bothWords[0][1:-1]] = bothWords[1][:-1]

    labels = []

    counter = 0

    # create a labels array that matches the xs array
    for f_name in allFileNames:
        # small number of files code:
        counter += 1
        if (limitNumFiles and counter > numFiles):
            break
        # the numbers here are based on removing "TextNumbers.txt" or "hexNumbers.txt" from the end of the files
        labels.append(dictionaryLabels[f_name[:-19]])

    print("total lables count is: ", len(labels))

    # turn the strings into lists of integers
    xs = list(list(int(w) for w in xelem.split()) for xelem in xs)
    xs = np.array(xs)

    print("xs 1 len:", len(xs))

    # turn the strings into integers
    labels = list(int(w) for w in labels)
    labels = np.array(labels)

    # randomly sort the xs and labels arrays in the same way
    np.random.seed(seed)
    indices = np.arange(len(xs))
    np.random.shuffle(indices)
    xs = xs[indices]
    labels = labels[indices]

    print("xs 2 len:", len(xs))

    # add a start character to the beginning of each file array and increase all the file integers by index_from

    if start_char is not None:
        xs = [[start_char] + [w + index_from for w in x] for x in xs]
    elif index_from:
        xs = [[w + index_from for w in x] for x in xs]

    print("xs 3 len:", len(xs))

    # remove integers after maxlen length in each file
    xs_tmp = []
    if maxlen:
        # xs, labels = _remove_long_seq(maxlen, xs, labels)
        for x in xs:
            if len(x) > maxlen:
                xs_tmp.append(x[:maxlen])
            else:
                xs_tmp.append(x)
        xs = xs_tmp
        if not xs:
            raise ValueError('After filtering for sequences shorter than maxlen=' +
                             str(maxlen) + ', no sequence was kept. '
                                           'Increase maxlen.')
    if not num_words:
        num_words = max([max(x) for x in xs])

    print("xs 4 len:", len(xs))

    # by convention, use 2 as OOV word
    # reserve 'index_from' (=3 by default) characters:
    # 0 (padding), 1 (start), 2 (OOV)


    if oov_char is not None:
        xs = [[w if (skip_top <= w < num_words) else oov_char for w in x] for x in xs]
    else:
        xs = [[w for w in x if skip_top <= w < num_words] for x in xs]


    print("xs 5 len:", len(xs))
    print("total data count before split train and test is: ", len(xs))

    np.savez("G:/数据集/Microsoft/RunningOutput/np_train_data/hex_text_np_x_y_data_all_10000_s3.npz",
             x=xs, y=labels)

    # x_train, x_test, y_train, y_test = train_test_split(xs, labels,test_size=0.2,random_state=0)
    #
    # if numOrHex:
    #     # np.savez("G:/数据集/Microsoft/RunningOutput/np_train_data/num_np_train_data.npz", x_train=x_train, x_test=x_test, y_train=y_train,
    #     #      y_test=y_test)
    #     np.savez("G:/数据集/Microsoft/RunningOutput/np_train_data/hex_text_np_train_data_80-20.npz",
    #              x_train=x_train, x_test=x_test, y_train=y_train,
    #              y_test=y_test)
    # else:
    #     np.savez("G:/数据集/Microsoft/RunningOutput/np_train_data/hex_np_train_data.npz", x_train=x_train, x_test=x_test, y_train=y_train,
    #          y_test=y_test)
    #
    # print("finished quantifying files at ", time.time())
    #
    # return (x_train, y_train), (x_test, y_test)


if __name__ == '__main__':
    load_data(maxlen=20000, num_words=10000)
